\section{Asymptotic complexity}
\label{sec:asympt-compl}
% Many algorithms below rely on fast multiplication; thus, we let $\Mult
% : \N \rightarrow \N$ be a {\em multiplication function}, such that
% polynomials in $\F_p[X]$ of degree less than $n$ can be multiplied in
% $\Mult(n)$ operations, under the conditions of~\cite[Ch.~8.3]{vzGG}.
% Typical orders of magnitude for $\Mult(n)$ are $O(n^{\log_2(3)})$ for
% Karatsuba multiplication or $O(n\log (n) \log\log (n))$ for FFT
% multiplication. Using fast multiplication, fast algorithms are
% available for Euclidean division or extended GCD~\cite[Chapter~9 \&
% 11]{vzGG}.

% The cost of {\em modular composition}, that is, of computing $F(G)
% \bmod H$, for $F,G,H\in\F_p[X]$ of degrees at most $n$, will be
% written $\ModComp(n)$. We refer to~\cite[Chapter~12]{vzGG} for a
% presentation of known results in an algebraic computational model: the
% best known algorithms have subquadratic (but superlinear) cost in
% $n$. Note that in a boolean RAM model, the algorithm of~\cite{KeUm08}
% takes quasi-linear time.

\section{Fundamental algorithms}
\label{sec:fund-algor}
In this section we review some fundamental algorithms that we will
repeatedly use in the rest of the document. Most of the algorithms we
present are taken from~\cite{vzGG}; another source of inspiration
is~\cite{poly-formel}.

\subsection{Polynomial multiplication}
\label{sec:polyn-mult}
Multiplication of polynomials with coefficients in a ring is a
fundamental brick to which most of the algorithms in computer algebra
reduce.

In the previous section we introduced the notation $\Mult(n)$ to
denote the number of operations in $R$ required to multiply two
polynomials of degree at most $n$ in $R[X]$.  Using the school-book
algorithm, we have $\Mult(n) \in O(n^2)$. The first major step forward
in the complexity of multiplication was done by \index{Karatsuba
  multiplication}Karatsuba~\cite{karatsuba}. He observed that using
the formula
\begin{gather*}
  f = f_1X^n + f_2\text{,}\qquad g = g_1X^n + g2\text{,}\\
  fg = f_1g_1X^{2n} + \bigl((f_1+f_2)(g_1+g_2)-f_1g_1-f_2g_2\bigr)X^n + f_2g_2
  \text{,}
\end{gather*}
multiplication can be computed recursively using only $3$ recursive
calls.  It follows that $\Mult(n)\in O(n^{\log_23})$.

When the base ring $R$ is a field containing a primitive $n$-th root
of unit $\omega$, polynomials can be multiplied by evaluating at the
powers of $\omega$, multiplying each evaluation, and interpolating
back. The map that sends a polynomial of degree $n$ over its
evaluations at the $n$-th roots of unit is called
\index{discrete~Fourier~transform}\textbf{discrete Fourier transform},
there are many algorithms of complexity $O(n\log n)$ to compute it,
they all go under the generic name of
\index{FFT}\index{fast~Fourier~transform}\textbf{fast Fourier transform}
(FFT).

Thus, multiplication in certain fields can be carried out in time
$O(n\log n)$. In the famous paper~\cite{schonage+strassen}, Schönage
and Strassen showed how performing an FFT in an extension ring of $R$
containing the required roots of unit yields an algorithm of
complexity $O(n\log n\log\log n)$ to multiply polynomials in $R[X]$.

\subsection{Formal power series}
\label{sec:formal-power-series}
We denote by $R[[X]]$ the ring of
\index{formal~power~series}\textbf{formal power series} on $R$. Its
elements are the sequences $(f_i)_{i>0}$ of elements of $R$, they are
denoted by
\begin{equation}
  \label{eq:197}
  f(X) = \sum_{i>0}f_iX^i
  \text{.}
\end{equation}
Multiplication and evaluation are defined in the obvious way. An
element $f\in R[[X]]$ is invertible if and only if $f(0)$ is an unit
of $R$.

Since formal power series are infinite objects, to be used in a
discrete algorithm they must be approximated. We denote by $f\bmod
X^n$ the polynomial
\begin{equation}
  \label{eq:198}
  f\bmod X^n = \sum_{0\le i < n}f_iX^i
  \text{.}
\end{equation}
We write $f = g + O(X^n)$, where $g$ is a polynomial or a power
series, whenever
\[f\bmod X^n=g\bmod X^n\text{,}\] and we say that $g$ approximates $f$
to the precision $n$.

Using polynomial multiplication, the product of two series known up to
precision $n$ can be computed in $O(\Mult(n))$ operations.

\paragraph{Derivative, integral}
\label{sec:derivative-integral}
If $R$ contains $\Q$, we define the
\index{formal~power~series!derivative}derivative and the
\index{formal~power~series!integral}integral of a power series as
\begin{align}
  \label{eq:200}
  f'(X) &= \sum_{i\ge0}(i+1)f_{i+1}X^i\text{,}\\
  \int f(X) &= \sum_{i\ge0}\frac{f_i}{i+1}X^{i+1}\text{.}
\end{align}
Derivatives and integrals up to precision $n$ can be computed in
$O(n)$ operations by their definition.

\paragraph{Logarithm, exponential}
\label{sec:logarithm}
The \index{formal~power~series!logarithm}logarithm of a power series
$f$ such that $f(0)=1$ is defined as
\begin{equation}
  \label{eq:194}
  \log f = \int\frac{f'}{f}
  \text{.}
\end{equation}
The \index{formal~power~series!exponential}exponential of a power
series $f$ such that $f(0)=0$, is defined as
\begin{equation}
  \label{eq:195}
  \exp(f) = 1 + f/1! + f^2/2! + \cdots
\end{equation}

\paragraph{First order linear differential equations}
\label{sec:first-order-linear}
All the usual identities involving multiplication, derivatives,
integrals, logarithms and exponentials are verified on power
series. An immediate consequence of this is a formula to solve first
order linear differential equations to Brent and
Kung~\cite{brent+kung}.

Let $f,g\in
R[[X]]$, the equation
\begin{equation}
  \label{eq:208}
  y' = f(X)y + g(X)
\end{equation}
with initial condition $y(0)=a$ has solution
\begin{equation}
  \label{eq:209}
  y(X) =  \frac{1}{j(X)}\left( a + \int g(X)j(X)\right)
  \text{,}
\end{equation}
where $j = \exp(-\int f)$; the verification is immediate.

In the next subsection we shall see that multiplicative inverses,
logarithms, exponentials and powers up to precision $n$ can all be
computed in time $O(\Mult(n))$, thus formula~\eqref{eq:209} can also
be applied at the same cost.


\begin{nota}
  If $R$ does not contain $\Q$, but has characteristic $0$, it is easy
  to use the previous definitions by working in
  $R[2^{-1},3^{-1},\ldots]$ and taking the result back in $R$ when
  needed. In characteristic different from $0$, these definition do
  not make sense anymore because Eq.~\eqref{eq:200} introduces a
  division by $0$. However, when $2,3,\ldots,n$ are invertible in $R$,
  we can still do computations on power series truncated to the order
  $n$.
\end{nota}


\subsection{Newton's iteration}
\label{sec:newtons-iteration}
Let $\Phi:\R\ra\R$ be a $C^1$ function, the
\index{Newton's~iteration}Newton's iteration is a classical method to
approximate a root $x$ of $\Phi$. Start from an approximation $x_0$, and
\emph{linearize} $\Phi$ to compute
\begin{equation}
  \label{eq:192}
  x_1 = x_0 - \frac{\Phi(x_0)}{\Phi'(x_0)}
  \text{,}
\end{equation}
then iterate this step until the desired precision is obtained. When
$x_0$ is taken close enough to a root, and when the derivative at this
root is non-zero, Newton's iteration converges \emph{quadratically} to
the solution, meaning that at each iteration the distance to the
solution is squared.

In computer algebra, Newton's iteration is applied to operators
$\Phi:R[[X]]\ra R[[X]]$ on formal power series; in this context,
\emph{quadratic} convergence means that the number of correct terms is
doubled at each iteration. Many fast algorithms for some fundamental
operations on power series and polynomials are obtained by this
method, here we summarize the most important ones.

\paragraph{Inversion}
If $f\in R[[X]]$ is invertible, the operator $\Phi(y) = 1/y - f$
applied to $y_0=1$ converges quadratically to the inverse of
$f$. Since the iteration associated to $\Phi$ is
\begin{equation}
  \label{eq:193}
  y_{i+1} = y_i(2 - y_if)
  \text{,}
\end{equation}
the cost of inverting a power series is $O(\Mult(n))$. From
Eq.~\eqref{eq:194} we deduce that computing the logarithm of a power
series has the same cost.

Another important consequence of this algorithm is that the Euclidean
division of polynomials of degree at most $n$ can also be performed in
$O(\Mult(n))$ operations.

\paragraph{Exponential}
If $f$ is such that $f(0)=0$, we compute its exponential using the
operator $\Phi(y)=f-\log y$, which gives the iteration
\begin{equation}
  \label{eq:196}
  y_{i+1} = y_i(1 + f - \log y)
  \text{.}
\end{equation}
Thus, the cost of computing an exponential is $O(\Mult(n))$ too. Using
the formula
\begin{equation}
  \label{eq:201}
  f^\alpha = \exp(\alpha\log f)
  \text{,}
\end{equation}
we deduce that, in characteristic $0$, computing arbitrary rational
powers of power series costs $O(\Mult(n))$ too.


\subsection{Modular composition}
\label{sec:modular-composition}
Given polynomials $f,g,h\in R[X]$ of degree at most $n$, the
\index{modular~composition}\emph{modular composition} requires to
compute
\begin{equation}
  \label{eq:190}
  f(g(X)) \mod h(X)
  \text{;}
\end{equation}
the special case where $h=X^n$ permits to compute the
\index{formal~power~series!composition}\emph{composition of power
  series} truncated to the order $n$.

Modular composition is a fundamental algorithm with lots of
applications, the most relevant being polynomial
factorization\cite{vzgathen+shoup92,kaltofen+shoup98} and computation
of minimal polynomials (see Remark~\ref{rk:shoups-algorithm-1}). In
the next subsection we shall also see an application of composition of
power series to solve differential equations.

Since many algorithms in this document make use of modular
composition, we introduced the notation $\ModComp(n)$ for its
complexity. A naive algorithm implies $\ModComp(n)\in
O(n\Mult(n))$. The first improvement to this bound was given by Brent
and Kung in~\cite{brent+kung}: they devise a baby step-giant step
algorithm of complexity $O\left(\sqrt{n}\Mult(n) +
  n^{\frac{\omega+1}{2}}\right)$; in the same paper they also gave an
algorithm of complexity $O\left(\sqrt{n\log n}\Mult(n)\right)$ for
composition of power series. Bernstein~\cite{bernstein98} found the
bound $O(\Mult(n)\log n)$ for the composition of power series in case
the characteristic of the base ring is small, however for a long time
Brent and Kung's algorithm and its
variants\cite{huang+pan98,kaltofen+shoup98} have stood as the only
generic algorithm for modular composition. A major breakthrough has
been recently achieved by Kedlaya and
Umans~\cite{umans:08,kedlaya+umans08}, who give an algorithm for
modular composition over a finite field $\F_q$ of \emph{binary}
complexity $n^{1+O(1)}\log^{1+O(1)}q$, using a reduction to
multivariate multipoint evaluation.


\paragraph{Computing iterated Frobenius and pseudotrace}
\label{sec:comp-frob-trace}
Fast modular composition can be used to compute Frobenius
automorphisms and \emph{pseudotraces} in finite fields. This algorithm
is due to von zur Gathen and Shoup~\cite{vzgathen+shoup92}, who
applied it to polynomial factorization. We will repeatedly use it in
Chapters~\ref{cha:artin-schr-towers} and~\ref{cha:algor-small-char}.

Consider the field extension $\F_{q^d}/\F_q$, its Galois group is
generated by the Frobenius automorphism
\begin{equation}
  \label{eq:199}
  \begin{aligned}
  \frob_q : \F_{q^d}&\ra\F_{q^d}\text{,}\\
  x &\mapsto x^{q}\text{.}
  \end{aligned}
\end{equation}
For any $n<d$, we also define the $n$-th
\index{pseudotrace}\emph{pseudotrace}\footnote{In~\cite{vzgathen+shoup92},
  this map goes under the name of \index{trace~map}\emph{trace map}.}
as
\begin{equation}
  \label{eq:202}
  \begin{aligned}
    \PTr_n : \F_{q^d}&\ra\F_{q^d}\text{,}\\
    x&\mapsto\sum_{i=0}^{n-1} x^{q^i}\text{.}
  \end{aligned}
\end{equation}
Notice that, when $n=d$, the pseudotrace coincides with the trace
$\Tr_{\F_{q^d}/\F_q}$; in this case one can use much faster
algorithms.

We suppose that elements of $\F_{q^d}$ are represented as residue
classes in $\F_q[X]/f(X)$ for some irreducible polynomial $f$, then
the Frobenius morphism can be computed with $O(\log q)$
multiplications in $\F_{q^d}$ plus one modular composition as
\begin{align}
  \label{eq:203}
  \Phi_1(X) &= X^q \bmod f(X)\text{,}\\
  \frob_q(a) &= X^q\circ a\bmod f = a\circ X^q\bmod f = a\circ \Phi_1\bmod f
  \text{.}
\end{align}


\begin{algorithm}
  \label{alg:itfrob}
  \caption{Iterated Frobenius}
  \begin{algorithmic}[1]
    \REQUIRE $0<i<d$, $a\in\F_q[X]/f(x)$, $\Phi_1(X) = X^q\bmod f(x)$.
    \ENSURE $\frob_q^i(a)$.
    \STATE let $i=\sum b_j2^j$ be the binary expansion of $i$;
    \STATE $k \la 1$;
    \FOR {$j=\lfloor\log_2 i\rfloor -1$ \TO $0$}
    \IF{$b_j=0$}
    \STATE $\Phi_{2k} \la \Phi_k\circ\Phi_k \bmod f$;
    \STATE $k \la 2k$;
    \ELSE
    \STATE $\Phi_{2k} \la \Phi_k\circ\Phi_k \bmod f$;
    \STATE $\Phi_{2k+1} \la \Phi_{2k}\circ\Phi_1 \bmod f$;
    \STATE $k \la 2k +1$;
    \ENDIF
    \ENDFOR
    \STATE return $a\circ\Phi_i \bmod f$.
  \end{algorithmic}
\end{algorithm}

Iterating $i$ times the $\frob_q$ can be done with only $O(\log i)$
modular compositions via square-and-multiply as shown in
Algorithm~\ref{alg:itfrob}.

Thus the cost of computing the $i$-th iterated Frobenius is
\begin{equation}
  \label{eq:204}
  O(\ModComp(d)\log i)
\end{equation}
operations in $\F_q$ plus a precomputation costing $O(\Mult(d)\log
q)$.

\begin{algorithm}
  \caption{Pseudotrace}
  \begin{algorithmic}[1]
    \REQUIRE $0<i<d$, $a\in\F_q[X]/f(x)$, $\Phi_1(X) = X^q\bmod f(x)$.
    \ENSURE $\PTr_n(a)$.
    \STATE let $n=\sum b_j2^j$ be the binary expansion of $n$;
    \STATE $\Theta_o \la 0$, $\Theta_1 \la a\circ\Phi_{1}$;
    \STATE $k=b_0$;
    \FOR{$j=1$ \TO $\lfloor\log_2n\rfloor$}
    \STATE $\Phi_{2^j} \la \Phi_{2^{j-1}}\circ\Phi_{2^{j-1}} \bmod f$;
    \STATE $\Theta_{2^j} \la \Theta_{2^{j-1}} + \Theta_{2^{j-1}}\circ\Phi_{2^{j-1}} \bmod f$;
    \IF{$b_j=1$}
    \STATE $\Theta_{2^j+k} \la \Theta_{2^j} + \Theta_{k}\circ\Phi_{2^j} \bmod f$;
    \STATE $k \la 2^j + k$;
    \ENDIF
    \ENDFOR
    \STATE return $\Theta_n$.
  \end{algorithmic}
\end{algorithm}

We apply the same idea to compute the $n$-th pseudotrace in time
$O(\ModComp(d)\log n)$; note that we use a dynamic programming
technique to keep the complexity into this bound. The key equation is
\begin{equation}
  \label{eq:205}
  \PTr_{n+m}(a) = \PTr_{n}(a)+\frob_q^n(\PTr_{m}(a))
  \text{.}
\end{equation}


\subsection{Chinese remainder algorithm and interpolation}
\label{sec:chin-rema-algor}

\cite[$\S$10]{vzGG}




\subsection[XGCD, Cauchy interpolation and RFR]{Euclidean algorithm,
  Cauchy interpolation and rational fraction reconstruction}
\label{sec:eucl-algor-rati}
Let $\K$ be a field, given two polynomials $f,g\in\K[X]$ of degrees
$m,n$, the \index{Euclidean~algorithm}Euclidean algorithm permits to
compute their \index{GCD}GCD using $O(mn)$ operations in $\K$. Let $r$
be the GCD of $f$ and $g$, a \index{Bézout~relation}\emph{Bézout
  relation} is an equation of the form
\begin{equation}
  \label{eq:154}
  fu + gv = r
  \text{,}
\end{equation}
with $u,v\in\K[X]$. If we ask $\deg(ur)<\deg(g)$ and
$\deg(vr)<\deg(a)$, the Bézout relation is unique; computing it is
called the extended GCD problem (\index{XGCD}XGCD) and can be computed
by the \index{extended Euclidean algorithm}\emph{extended Euclidean
  algorithm}.

\begin{algorithm}
  \caption{Extended Euclidean algorithm}
  \begin{algorithmic}[1]
    \REQUIRE $f,g\in\K[X]$.
    \ENSURE $u,v,r\in\K[X]$ such that $fu+gv=r$.
    \STATE let $r_0\la f$, $u_0\la1$, $v_0\la0$;
    \STATE let $r_1\la f$, $u_1\la0$, $v_1\la1$;
    \STATE $i\la1$;
    \WHILE{$r_i\ne0$}
    \STATE compute $r_{i-1} = q_ir_i + r_{i+1}$ by Euclidean division;
    \STATE compute $u_{i+1} \la u_{i-1} - q_iu_i$, $v_{i+1} \la v_{i-1} - q_iv_i$;
    \STATE $i\la i+1$;
    \ENDWHILE
    \STATE return $u_i,v_i,r_i$.
  \end{algorithmic}
\end{algorithm}

One important application of XGCD's is computing modular inverses. Let
$f,g\in\K[X]$ with $\deg(g)<\deg(f)$ and $f$ prime to $g$, then $r$ is
a unit in $\K$, and a Bézout relation implies
\begin{equation}
  \label{eq:206}
  g\frac{v}{r} \equiv 1 \mod f
  \text{.}
\end{equation}

More generally, the polynomials computed at each iteration by the
extended Euclidean algorithm satisfy
\begin{equation}
  \label{eq:156}
  fu_i + gv_i =  r_i
  \qquad\text{for any $i$;}
\end{equation}
each of these is also called a Bézout relation. These relations have
two major applications: \emph{Cauchy interpolation} and \emph{rational
  fraction reconstruction}.

Given $n$ pairs $(x,e)\in\K\times\K$ with all $x$ distinct and an
integer $\ell<n$, \index{Cauchy~interpolation}Cauchy interpolation
computes, if it exists, a rational fraction $\frac{r}{v}\in\K(X)$ with
$\deg r<\ell$ and $\deg v \le n-\ell$, such that $\frac{r(x)}{v(x)}=e$
for any $(x,e)$. Let $f=\prod (X-x)$, by interpolation one obtains the
unique polynomial $g\in\K[X]/f$ such that $g(x)=e$ for any
$(x,e)$. Then a Bézout relation for $f$ and $g$ gives
\begin{equation}
  \label{eq:207}
  \frac{r_i}{v_i} \equiv g \mod f
  \qquad\text{for any $i$,}
\end{equation}
thus in particular $\frac{r_i(x)}{v_i(x)}=e$ for any $(x,e)$. It can
be proven that a solution to the Cauchy interpolation problem exists
if and only if one of the intermediate results of the extended
Euclidean algorithm is such that $\deg(r_i)<\ell$ and $\deg(v_i)\le
n-\ell$.

\index{rational~fraction~reconstruction}Rational fraction
reconstruction (\index{RFR}RFR) is very similar to Cauchy
interpolation, and it can be viewed as a generalization of it using
multiplicities. Let $g\in\K[[X]]$ be a power series, we want to
compute a rational fraction $\frac{r}{v}\in\K(X)$ with $\deg(r)<\ell$
and $\deg(v)\le n-\ell$, such that $\frac{r}{v}=g+O(X^n)$ in
$\K[[x]]$. Such a rational fraction is called a
\index{Padé~approximant}\emph{Padé approximant} of type
$(\ell-1,n-\ell)$ of $g$. Again, it can be shown that a Padé
approximant of type $(\ell-1,n-\ell)$ exists if and only if
\begin{equation}
  \label{eq:210}
  \frac{r_i}{v_i}\equiv g \mod X^{n+m+1}
\end{equation}
is one of the intermediate results computed by the extended Euclidean
algorithm.

The extend Euclidean algorithm is not optimal. We address the reader
to~\cite[$\S$11.1]{vzGG} for the description of an algorithm that
takes $f,g\in\K[X]$ of degree at most $n$ and $\ell\le n$, and
computes, using $O(\Mult(n)\log n)$ operations, the rows $u_i,v_i,r_i$
and $u_{i+1},v_{i+1},r_{i+1}$ of the Extended Euclidean algorithm such
that $\deg(r_i)\ge n-\ell$ and $\deg(r_{i+1})<n-\ell$. A consequence of
this algorithm is that both Cauchy interpolation and rational fraction
reconstruction can be computed in time $O(\Mult(n)\log n)$.



\subsection{Multivariate polynomials}
\label{sec:mult-polyn}
Kronecker substitution (vzGS92)\\
maybe bivariate operations (some Pascal-Schost and some Li-Moreno-Schost) ?

\subsection{Transposed algorithms}
\label{sec:transp-algor}
transposed mul, transposed mod,


%%% Local Variables: 
%%% mode:flyspell
%%% ispell-local-dictionary:"american"
%%% mode: TeX-PDF
%%% mode: reftex
%%% TeX-master: "../these"
%%% End: 
